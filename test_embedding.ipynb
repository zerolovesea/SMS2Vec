{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5de826d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ./model/dynamic_vec/BAAI/bge-m3\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "# Download model from ModelScope\n",
    "model_dir = snapshot_download('BAAI/bge-m3', cache_dir='./model/dynamic_vec')\n",
    "\n",
    "# Load from local directory\n",
    "model = BGEM3FlagModel(model_dir, use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0286cb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pre tokenize: 100%|██████████| 874/874 [00:00<00:00, 1473.08it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Inference Embeddings: 100%|██████████| 874/874 [02:33<00:00,  5.70it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('/Users/zyaztec/Projects/SMS2Vec/data/raw/predict_data.csv')\n",
    "sentences = data['message'].tolist()\n",
    "print(len(sentences))\n",
    "embeddings = model.encode(sentences, batch_size=32, max_length=1024)['dense_vecs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a94dc450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27939, 1024)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfd8b3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ./model/dynamic_vec/Qwen/Qwen3-Embedding-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 15:56:17,071 - modelscope - INFO - Got 12 files, start to download ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219a0d959ce1421aa55632d73fbedc28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 12 items:   0%|          | 0.00/12.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd53e65e128444fb60f87d42cc9588e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [1_Pooling/config.json]:   0%|          | 0.00/313 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7738a2785344298eff9c50ba8596bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [config_sentence_transformers.json]:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7969c324f74734ac6a865155b0cf65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model.safetensors]:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1332a4ade223403c860e72153fc0e606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [merges.txt]:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81491c9ba4644a409aaebf683f01cddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [modules.json]:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1885a2ec67d4461eba78e5e32211d597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [config.json]:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd3c7f96f9a45718e016e8a636f5200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [configuration.json]:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de092a14c57547e9b32eb604c136183f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [generation_config.json]:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272dfd44b22a49129e81c9e80bf784c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [README.md]:   0%|          | 0.00/16.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea39ef73f6c42e185fb079dade75a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer.json]:   0%|          | 0.00/10.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432405c505cb43ed83c29688fb617fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer_config.json]:   0%|          | 0.00/9.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6dbd15ca009435882860af326a8f493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [vocab.json]:   0%|          | 0.00/2.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 16:38:59,856 - modelscope - INFO - Download model 'Qwen/Qwen3-Embedding-0.6B' successfully.\n",
      "2025-08-27 16:38:59,857 - modelscope - INFO - Creating symbolic link [./model/dynamic_vec/Qwen/Qwen3-Embedding-0.6B].\n"
     ]
    }
   ],
   "source": [
    "model_dir = snapshot_download('Qwen/Qwen3-Embedding-0.6B', cache_dir='./model/dynamic_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e14f6500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./model/dynamic_vec/Qwen/Qwen3-Embedding-0___6B'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01f3bb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7646, 0.1414],\n",
      "        [0.1355, 0.6000]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer(model_dir)\n",
    "\n",
    "\n",
    "# The queries and documents to embed\n",
    "queries = [\n",
    "    \"What is the capital of China?\",\n",
    "    \"Explain gravity\",\n",
    "]\n",
    "documents = [\n",
    "    \"The capital of China is Beijing.\",\n",
    "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n",
    "]\n",
    "\n",
    "# Encode the queries and documents. Note that queries benefit from using a prompt\n",
    "# Here we use the prompt called \"query\" stored under `model.prompts`, but you can\n",
    "# also pass your own prompt via the `prompt` argument\n",
    "query_embeddings = model.encode(queries, prompt_name=\"query\")\n",
    "document_embeddings = model.encode(documents)\n",
    "\n",
    "# Compute the (cosine) similarity between the query and document embeddings\n",
    "similarity = model.similarity(query_embeddings, document_embeddings)\n",
    "print(similarity)\n",
    "# tensor([[0.7646, 0.1414],\n",
    "#         [0.1355, 0.6000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbfd95df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1024)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c715fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
